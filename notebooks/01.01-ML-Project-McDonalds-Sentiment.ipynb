{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project with McDonald's sentiment data [in progress]\n",
    "\n",
    "by Georgios Karakostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaginary problem statement\n",
    "\n",
    "McDonald's receives **thousands of customer comments** on their website per day, and many of them are negative. Their corporate employees don't have time to read every single comment, but they do want to read a subset of comments that they are most interested in. In particular, the media has recently portrayed their employees as being rude, and so they want to review comments about **rude service**.\n",
    "\n",
    "McDonald's has hired us to develop a system that ranks each comment by the **likelihood that it is referring to rude service**. They will use our system to build a \"rudeness dashboard\" for their corporate employees, so that employees can spend a few minutes each day examining the **most relevant recent comments**.\n",
    "\n",
    "## Description of the data\n",
    "\n",
    "Before hiring us, McDonald's used the [CrowdFlower platform](http://www.crowdflower.com/data-for-everyone) to pay humans to **hand-annotate** about 1500 comments with the **type of complaint**. The complaint types are listed below, with the encoding used in the data listed in parentheses:\n",
    "\n",
    "- Bad Food (BadFood)\n",
    "- Bad Neighborhood (ScaryMcDs)\n",
    "- Cost (Cost)\n",
    "- Dirty Location (Filthy)\n",
    "- Missing Item (MissingFood)\n",
    "- Problem with Order (OrderProblem)\n",
    "- Rude Service (RudeService)\n",
    "- Slow Service (SlowService)\n",
    "- None of the above (na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "We read **`mcdonalds.csv`** into a pandas DataFrame and examine it. \n",
    "\n",
    "- The **policies_violated** column lists the type of complaint. If there is more than one type, the types are separated by newline characters.\n",
    "- The **policies_violated:confidence** column lists CrowdFlower's confidence in the judgments of its human annotators for that row (higher is better).\n",
    "- The **city** column is the McDonald's location.\n",
    "- The **review** column is the actual text comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mcdonalds.csv using a relative path\n",
    "import pandas as pd\n",
    "path = '../data/mcdonalds.csv'\n",
    "mcd = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>policies_violated:confidence</th>\n",
       "      <th>city</th>\n",
       "      <th>policies_violated_gold</th>\n",
       "      <th>review</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:36</td>\n",
       "      <td>RudeService\\nOrderProblem\\nFilthy</td>\n",
       "      <td>1.0\\n0.6667\\n0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:27</td>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Terrible customer service. ŒæI came in at 9:30...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:26</td>\n",
       "      <td>SlowService\\nOrderProblem</td>\n",
       "      <td>1.0\\n1.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  679455653    False   finalized                   3      2/21/15 0:36   \n",
       "1  679455654    False   finalized                   3      2/21/15 0:27   \n",
       "2  679455655    False   finalized                   3      2/21/15 0:26   \n",
       "\n",
       "                   policies_violated policies_violated:confidence     city  \\\n",
       "0  RudeService\\nOrderProblem\\nFilthy          1.0\\n0.6667\\n0.6667  Atlanta   \n",
       "1                        RudeService                            1  Atlanta   \n",
       "2          SlowService\\nOrderProblem                     1.0\\n1.0  Atlanta   \n",
       "\n",
       "   policies_violated_gold                                             review  \\\n",
       "0                     NaN  I'm not a huge mcds lover, but I've been to be...   \n",
       "1                     NaN  Terrible customer service. ŒæI came in at 9:30...   \n",
       "2                     NaN  First they \"lost\" my order, actually they gave...   \n",
       "\n",
       "   Unnamed: 10  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first three rows\n",
    "mcd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not a huge mcds lover, but I've been to better ones. This is by far the worst one I've ever been too! It's filthy inside and if you get drive through they completely screw up your order every time! The staff is terribly unfriendly and nobody seems to care.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the text of the first review\n",
    "mcd.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "We remove any rows from the DataFrame in which the **policies_violated** column has a **null value**. We check the shape of the DataFrame before and after to confirm that we only removed about 50 rows.\n",
    "\n",
    "- **Note:** Null values are also known as \"missing values\", and are encoded in pandas with the special value \"NaN\". This is distinct from the \"na\" encoding used by CrowdFlower to denote \"None of the above\". Rows that contain \"na\" should **not** be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape before removing any rows\n",
    "mcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                           0\n",
       "_golden                            0\n",
       "_unit_state                        0\n",
       "_trusted_judgments                 0\n",
       "_last_judgment_at                  0\n",
       "policies_violated                 54\n",
       "policies_violated:confidence      54\n",
       "city                              87\n",
       "policies_violated_gold          1525\n",
       "review                             0\n",
       "Unnamed: 10                     1525\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of null values in each column\n",
    "mcd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the DataFrame to only include rows in which policies_violated is not null\n",
    "mcd = mcd[mcd.policies_violated.notnull()]\n",
    "\n",
    "# alternatively, use the 'dropna' method to accomplish the same thing\n",
    "mcd.dropna(subset=['policies_violated'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape after removing rows\n",
    "mcd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "We add a new column to the DataFrame called **\"rude\"** that is 1 if the **policies_violated** column contains the text \"RudeService\", and 0 if the **policies_violated** column does not contain \"RudeService\". The \"rude\" column is going to be our response variable, so we should check how many zeros and ones it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the string 'RudeService', and convert the boolean results to integers\n",
    "mcd['rude'] = mcd.policies_violated.str.contains('RudeService').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>rude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RudeService\\nOrderProblem\\nFilthy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SlowService\\nOrderProblem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   policies_violated  rude\n",
       "0  RudeService\\nOrderProblem\\nFilthy     1\n",
       "1                        RudeService     1\n",
       "2          SlowService\\nOrderProblem     0\n",
       "3                                 na     0\n",
       "4                        RudeService     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that it worked\n",
    "mcd.loc[0:4, ['policies_violated', 'rude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    968\n",
       "1    503\n",
       "Name: rude, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "mcd.rude.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "1. We define X (the **review** column) and y (the **rude** column).\n",
    "2. We split X and y into training and testing sets (using the parameter **`random_state=1`**).\n",
    "3. We use CountVectorizer (with the **default parameters**) to create document-term matrices from X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = mcd.review\n",
    "y = mcd.rude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103, 7300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and transform X_train into X_train_dtm\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368, 7300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform X_test into X_test_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "We fit a Multinomial Naive Bayes model to the training set, calculate the **predicted probabilites** (not the class predictions) for the testing set, and then calculate the **AUC**. We will repeat this task using a logistic regression model to see which of the two models achieves a better AUC.\n",
    "\n",
    "- **Note:** Because McDonald's only cares about ranking the comments by the likelihood that they refer to rude service, **classification accuracy** is not the relevant evaluation metric. **Area Under the Curve (AUC)** is a more useful evaluation metric for this scenario, since it measures the ability of the classifier to assign higher predicted probabilities to positive instances than to negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import/instantiate/fit a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the predicted probability of rude=1 for each testing set observation\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8426005404546177"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the AUC\n",
    "from sklearn import metrics\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8233667143538389"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat this task using a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Since Naive Bayes has a better AUC (see previous step), we use it to try **tuning CountVectorizer**. We check the testing set **AUC** after each change, and find the set of parameters that increases AUC the most.\n",
    "\n",
    "- **Note:** To help us in this process, we will define a **`tokenize_test()`** function that will allow us to iterate quickly through different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the AUC\n",
    "def tokenize_test(vect):\n",
    "    \n",
    "    # create document-term matrices using the vectorizer\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    # print the number of features that were generated\n",
    "    print('Features:', X_train_dtm.shape[1])\n",
    "    \n",
    "    # use Multinomial Naive Bayes to calculate predicted probabilities\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "    \n",
    "    # print the AUC\n",
    "    print('AUC:', metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 7300\n",
      "AUC: 0.8426005404546177\n"
     ]
    }
   ],
   "source": [
    "# confirm that the AUC is identical to task 5 when using the default parameters\n",
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1732\n",
      "AUC: 0.8621522810364012\n"
     ]
    }
   ],
   "source": [
    "# tune CountVectorizer to increase the AUC\n",
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
