{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Gradient  Descent \n",
    "\n",
    "by Georgios Karakostas\n",
    "\n",
    "- **Goal:** Mathematically implement univariate and multivariate linear regression using gradient descent algorithm. \n",
    "\n",
    "## Linear Regression with One Variable\n",
    "\n",
    "First, some context on the problem statement.\n",
    "\n",
    "In this first part of the notebook, we will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\n",
    "\n",
    "The file `ex1data1.txt` (available in the `data` subdirectory) contains the data set for our linear regression exercise. The first column is the **population** of a city and the second column is the **profit** of a food truck in that city. A negative value for profit indicates a loss.\n",
    "\n",
    "First, as with doing any Machine Learning task, we need to import certain libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Plotting our Data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, we can use a *scatter plot* to visualize the data, since it has only two properties to plot, i.e. **population** and **profit** which are the feature and target, respectively.\n",
    "\n",
    "- **Note:** Many other problems that we will encounter in real life are multi-dimensional and can’t be plotted on a $2d$ plot. To create multidimensional plots we have to be creative in using various aesthetics like colors, shapes, depths, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e0d0c30103dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/ex1data1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv('../data/ex1data1.txt', header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.iloc[:, 0]\n",
    "y = data1.iloc[:, 1]\n",
    "m = len(y) # number of training examples\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Intercept Term\n",
    "\n",
    "In the following lines of code, we add another dimension to our data to accommodate the intercept term. We also initialize the initial parameters $\\theta$ to $0$ and the learning rate $\\alpha$ to $0.01$.\n",
    "\n",
    "- **Note:** Recall that the shape of rank-1 arrays is of the form: $(m, )$. On the other hand, the shape of rank-2 arrays is of the form: $(m, 1)$. Now, when we read our data into `X` and `y`, we obserbe that both `X` and `y` are rank-1 arrays. When operating on arrays, its good practice to convert rank-1 arrays to rank-2 arrays as rank-1 arrays often give unexpected results. To implement the conversion, we use the following Numpy command: `someArray[:, np.newxis]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[:, np.newaxis]\n",
    "theta = np.zeros([2, 1])\n",
    "iterations = 1500\n",
    "alpha = 0.01 # learning rate\n",
    "ones = np.ones([m, 1]) \n",
    "X = np.hstack([ones, X])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note:** For the people who have experience with the Numpy library, I just want to point out that there is a much more elegant and quicker way to add another dimension to our data using the following code: `X = np.hstack([np.ones([m,1]), data.iloc[:,0][:, np.newaxis]])`.\n",
    "\n",
    "Next, we will compute the **cost function** $J$ and implement the **gradient descent** algorithm to find the optimal values of the parameters $\\theta_0, \\theta_1$. To that end, we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Cost Function\n",
    "\n",
    "We can measure the accuracy of our hypothesis function by using a **cost function**. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from `X`'s and the actual output `y`'s.\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    temp = np.dot(X, theta) - y\n",
    "    return np.sum(np.power(temp, 2)) / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameters $\\theta$ initialized to $(0, 0)$, the cost computed is approximately equal to $32.07$. \n",
    "\n",
    "We want to find the optimal parameter values such that the cost function is minimal. We will use the **gradient descent** algorithm to find the global minimum of *J*. To that end, we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal Parameters using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **gradient descent** algorithm is described mathematically as follows:\n",
    "\n",
    "Repeat until convergence \n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1),$$ \n",
    "\n",
    "where $j=0,1$ represents the feature index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    for _ in range(iterations):\n",
    "        temp = np.dot(X, theta) - y\n",
    "        temp = np.dot(X.T, temp)\n",
    "        theta = theta - (alpha / m) * temp\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = gradientDescent(X, y, theta, alpha, iterations)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opitmal parameter values found by the gradient descent algorithm are $\\theta_0 = -3.63$ and $\\theta_1 = 1.17$.\n",
    "\n",
    "Now that we have the optimized values of $\\theta_0$ and $\\theta_1$, we can plug them in the **cost function** $J$ and compare our final cost with the initial cost of $32.07$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot showing the Best Fit Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 1], y)\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.plot(X[:, 1], np.dot(X, theta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making some Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1 = np.dot([1, 3.5], theta)\n",
    "predict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = np.dot([1, 7], theta)\n",
    "predict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- For a population of $35,000$ inhabitants, our model predicts a profit of $4519,76\\$$.\n",
    "- For a population of $70,000$ inhabitants, our model predicts a profit of $45342.45\\$$.\n",
    "\n",
    "Next, lets extend the idea of linear regression to work with **multiple independent variables**. To that end, we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression \n",
    "\n",
    "In this section, we will implement linear regression with multiple variables (also called Multivariate Linear Regression).\n",
    "\n",
    "**Problem context:**\n",
    "Suppose you are selling our house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices. Your job is to predict housing prices based on other variables.\n",
    "The file `ex1data2.txt`(available in the `data` subdirectory) contains a training set of housing prices in Portland, Oregon. The first column is the **size of the house** (in square feet), the second column is the **number of bedrooms**, and the third column is the **price** of the house.\n",
    "\n",
    "We already have the necessary infrastructure which we built in our previous section that can be easily applied to this section as well. Here, we will just use the equations which we made in the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('../data/ex1data2.txt', sep = ',', header = None)\n",
    "X = data2.iloc[:,0:2] # read first two columns into X\n",
    "y = data2.iloc[:,2] # read the third column into y\n",
    "m = len(y) # number of training samples\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above we are dealing with more than one independent variables here (but the concepts we have implemented in the previous section applies here as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "By looking at the above DataFrame, we immediately note that house sizes are about 1000 times the number of bedrooms, i.e. $\\text{house size} \\approx 1000 \\times (\\text{# of bedrooms}$). When features differ by orders of magnitude, first performing **feature scaling** can make **gradient descent** converge much more quickly.\n",
    "\n",
    "Our following task is to implement **feature scaling** to our learning problem. Specifically, we will:\n",
    "\n",
    "- Subtract the mean value of each feature from the data set.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - np.mean(X))/np.std(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Intercept Term and Initializing Parameters\n",
    "\n",
    "- **Note:** The below code is similar to what we did in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones([m, 1])\n",
    "X = np.hstack([ones, X])\n",
    "alpha = 0.01\n",
    "num_iters = 400\n",
    "theta = np.zeros((3, 1))\n",
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    temp = np.dot(X, theta) - y\n",
    "    return np.sum(np.power(temp, 2)) / (2 * m)\n",
    "\n",
    "J = computeCostMulti(X, y, theta)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal Parameters using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, iterations):\n",
    "    for _ in range(iterations):\n",
    "        temp = np.dot(X, theta) - y\n",
    "        temp = np.dot(X.T, temp)\n",
    "        theta = theta - (alpha/m) * temp\n",
    "    return theta\n",
    "\n",
    "theta = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the optimized value of $\\theta_0, \\theta_1$ and $\\theta_2$. Let's use them in our **cost function** $J$ and compare our final cost with the initial cost of $65591548106.46$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = computeCostMulti(X, y, theta)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, $J_{final} < J_{initial}$. Our learning model is now optimized and we can use it to make predictions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
